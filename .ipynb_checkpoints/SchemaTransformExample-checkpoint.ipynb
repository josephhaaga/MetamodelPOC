{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema Transform Example\n",
    "In this example we will:\n",
    "\n",
    "1. ~~persist a source graph's metadata in Grakn~~\n",
    "2. perform a motif query to 'transform' the graph, and \n",
    "3. document the updated schema (versioning)\n",
    "\n",
    "Ideally, generated code snippets will apply the transforms to the source graph in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install graphframes\n",
    "# https://towardsdatascience.com/graphframes-in-jupyter-a-practical-guide-9b3b346cebc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, DataFrame\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark runtime boilerplate\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "sqlC = SQLContext(sc)\n",
    "sc.addPyFile(\"/Users/josephhaaga/.ivy2/jars/graphframes_graphframes-0.6.0-spark2.3-s_2.11.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Graph\n",
    "First we create a GraphFrame with the data generated by GeneratePeopleAndCompanies.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = sqlC.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"./data/peopleAndCompanies_edges.csv\") \n",
    "    \n",
    "vertices = sqlC.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"./data/peopleAndCompanies_vertices.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>company</td>\n",
       "      <td>Diaz, Sanchez and Williams</td>\n",
       "      <td>00807 Meadows Prairie Apt. 382 East Melaniebur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>company</td>\n",
       "      <td>Jones Inc</td>\n",
       "      <td>Unit 5674 Box 1556 DPO AA 42502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>company</td>\n",
       "      <td>Holmes Ltd</td>\n",
       "      <td>90746 Beasley Shoal Suite 136 New Joseph, WA 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>person</td>\n",
       "      <td>Margaret Khan</td>\n",
       "      <td>1533 Frederick Alley Jamesmouth, SD 62854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>person</td>\n",
       "      <td>Terry Herring</td>\n",
       "      <td>7340 Simmons Square Apt. 770 Isabellaville, MS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>person</td>\n",
       "      <td>Mariah Peterson</td>\n",
       "      <td>829 Combs Expressway Jamesstad, VT 68867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     type                        name  \\\n",
       "17  17  company  Diaz, Sanchez and Williams   \n",
       "18  18  company                   Jones Inc   \n",
       "19  19  company                  Holmes Ltd   \n",
       "20  20   person               Margaret Khan   \n",
       "21  21   person               Terry Herring   \n",
       "22  22   person             Mariah Peterson   \n",
       "\n",
       "                                              address  \n",
       "17  00807 Meadows Prairie Apt. 382 East Melaniebur...  \n",
       "18                    Unit 5674 Box 1556 DPO AA 42502  \n",
       "19  90746 Beasley Shoal Suite 136 New Joseph, WA 4...  \n",
       "20          1533 Frederick Alley Jamesmouth, SD 62854  \n",
       "21  7340 Simmons Square Apt. 770 Isabellaville, MS...  \n",
       "22           829 Combs Expressway Jamesstad, VT 68867  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices.toPandas()[17:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "# https://stackoverflow.com/a/50404308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example GraphFrame query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                   a|                   r|                   b|                  r2|                   c|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[22, person, Mari...|[22, 66, claims_d...|[66, person, Andr...|[66, 69, claims_d...|[69, person, Javi...|\n",
      "|[22, person, Mari...|[22, 88, claims_d...|[88, person, Caro...|[88, 34, claims_d...|[34, person, Tyle...|\n",
      "|[22, person, Mari...|[22, 88, claims_d...|[88, person, Caro...|[88, 92, claims_d...|[92, person, Anit...|\n",
      "|[22, person, Mari...|[22, 25, claims_d...|[25, person, Mary...|[25, 58, claims_d...|[58, person, Dona...|\n",
      "|[26, person, Thom...|[26, 45, claims_d...|[45, person, Terr...|[45, 47, claims_d...|[47, person, Geor...|\n",
      "|[29, person, Lisa...|[29, 71, claims_d...|[71, person, Evan...|[71, 58, claims_d...|[58, person, Dona...|\n",
      "|[35, person, Devi...|[35, 36, claims_d...|[36, person, Holl...|[36, 56, claims_d...|[56, person, Will...|\n",
      "|[40, person, Step...|[40, 64, claims_d...|[64, person, Alex...|[64, 98, claims_d...|[98, person, Paul...|\n",
      "|[49, person, Jame...|[49, 66, claims_d...|[66, person, Andr...|[66, 69, claims_d...|[69, person, Javi...|\n",
      "|[52, person, Jenn...|[52, 40, claims_d...|[40, person, Step...|[40, 64, claims_d...|[64, person, Alex...|\n",
      "|[57, person, Nich...|[57, 34, claims_d...|[34, person, Tyle...|[34, 20, claims_d...|[20, person, Marg...|\n",
      "|[63, person, Nico...|[63, 76, claims_d...|[76, person, Ange...|[76, 57, claims_d...|[57, person, Nich...|\n",
      "|[63, person, Nico...|[63, 57, claims_d...|[57, person, Nich...|[57, 34, claims_d...|[34, person, Tyle...|\n",
      "|[63, person, Nico...|[63, 57, claims_d...|[57, person, Nich...|[57, 44, claims_d...|[44, person, Shei...|\n",
      "|[63, person, Nico...|[63, 57, claims_d...|[57, person, Nich...|[57, 58, claims_d...|[58, person, Dona...|\n",
      "|[75, person, Lee ...|[75, 26, claims_d...|[26, person, Thom...|[26, 45, claims_d...|[45, person, Terr...|\n",
      "|[75, person, Lee ...|[75, 26, claims_d...|[26, person, Thom...|[26, 33, claims_d...|[33, person, Jack...|\n",
      "|[75, person, Lee ...|[75, 26, claims_d...|[26, person, Thom...|[26, 94, claims_d...|[94, person, Aman...|\n",
      "|[76, person, Ange...|[76, 57, claims_d...|[57, person, Nich...|[57, 34, claims_d...|[34, person, Tyle...|\n",
      "|[76, person, Ange...|[76, 57, claims_d...|[57, person, Nich...|[57, 44, claims_d...|[44, person, Shei...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dependents claiming dependents\n",
    "g.find(\"(a)-[r]->(b); (b)-[r2]->(c)\") \\\n",
    "    .filter(\"r.relationship == 'claims_dependent'\") \\\n",
    "    .filter(\"r2.relationship == 'claims_dependent'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'No such struct field type in src, dst, relationship; line 1 pos 0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o139.filter.\n: org.apache.spark.sql.AnalysisException: No such struct field type in src, dst, relationship; line 1 pos 0\n\tat org.apache.spark.sql.catalyst.expressions.ExtractValue$.findField(complexTypeExtractors.scala:85)\n\tat org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(complexTypeExtractors.scala:53)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:202)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:201)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$37.apply(Analyzer.scala:826)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$37.apply(Analyzer.scala:828)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:825)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:834)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:834)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:834)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:895)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:895)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:895)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:837)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:837)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:690)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:172)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:178)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:65)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3306)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1463)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1477)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b80d459fbfa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmotifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(a)-[e]->(b); (c)-[e2]->(b)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmotifs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a.type == 'person'\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"c.type == 'person'\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b.type == 'company'\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"e.type == 'employed_by'\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"e2.type == 'employed_by'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \"\"\"\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'No such struct field type in src, dst, relationship; line 1 pos 0'"
     ]
    }
   ],
   "source": [
    "motifs = g.find(\"(a)-[e]->(b); (c)-[e2]->(b)\")\n",
    "\n",
    "results = motifs.filter(\"a.type == 'person'\") \\\n",
    "    .filter(\"c.type == 'person'\") \\\n",
    "    .filter(\"b.type == 'company'\") \\\n",
    "    .filter(\"e.relationship == 'employed_by'\") \\\n",
    "    .filter(\"e2.relationship == 'employed_by'\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Source Graph in Grakn Metamodel\n",
    "The source graph, a GraphFrame named `g`, depicts a network of people claiming eachother as dependents. We need methods to extract the relevant features of this graph so that it can be depicted in the Grakn metamodel. Some examples of things we need to describe include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graphVertex\n",
    "- Attributes\n",
    "    - Name\n",
    "- Relationships\n",
    "    - has-type\n",
    "    - has-graphobjects\n",
    "    - has-concept\n",
    "    - has-vertexid\n",
    "    - has_property\n",
    "    - has-attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple list comprehension over the unique `types` of GraphFrame vertices can be translated into Graql `insert` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$person isa graphVertex has name \"person\";',\n",
       " '$company isa graphVertex has name \"company\";']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertexTypes = g.vertices.select(\"type\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "createVertices = ['$'+v+' isa graphVertex has name \"'+v+'\";' \\\n",
    " for v in vertexTypes]\n",
    "\n",
    "# insert statements may need to become match-insert statements if we want to update existing metamodel graphs\n",
    "\n",
    "createVertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graphEdge\n",
    "- Attributes\n",
    "    - Name\n",
    "- Relationships\n",
    "    - has-type\n",
    "    - has-graphobjects\n",
    "    - has-concept\n",
    "    - has-edgeids\n",
    "    - has_property\n",
    "    - has-attribute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple list comprehension over the unique `relationship` values of GraphFrame edges can be translated into Graql `insert` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$owned_by isa graphEdge has name \"owned_by\";',\n",
       " '$employed_by isa graphEdge has name \"employed_by\";',\n",
       " '$claims_dependent isa graphEdge has name \"claims_dependent\";']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgeTypes = g.edges.select(\"relationship\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "createEdges = ['$'+e+' isa graphEdge has name \"'+e+'\";' \\\n",
    " for e in edgeTypes]\n",
    "\n",
    "createEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graphTriplet\n",
    "We can query the GraphFrame for an explicit list of all triples in the graph, and `insert` relationships between the nodes and edges we've created.\n",
    "___\n",
    "We should try making `owned_by` an ambigious relationship. This can be changed in the GeneratePeopleAndCompanies.ipynb notebook.\n",
    "\n",
    "e.g. \n",
    "1. `(Company)-[owned_by]->(Company)`\n",
    "2. `(Company)-[owned_by]->(Person)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = g.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tripleTypes = s.join(vertices, s.src == vertices.id) \\\n",
    "    .select([\"src\",\"type\",\"dst\",\"relationship\"]) \\\n",
    "    .withColumnRenamed('type','src_type') \\\n",
    "    .join(vertices, s.dst == vertices.id) \\\n",
    "    .select([\"src\",\"src_type\",\"dst\",\"type\",\"relationship\"]) \\\n",
    "    .withColumnRenamed('type', 'dst_type') \\\n",
    "    .select(['src_type', 'relationship', 'dst_type']) \\\n",
    "    .distinct() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(src_type='company', relationship='owned_by', dst_type='company'),\n",
       " Row(src_type='person', relationship='employed_by', dst_type='company'),\n",
       " Row(src_type='person', relationship='claims_dependent', dst_type='person')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripleTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tripleTypes = [a.asDict() for a in tripleTypes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixing `match` and `insert` statements leads to syntax errors; let's try persisting the UUID for all the nodes/edges we create so we don't have to do any `match`ing during the triplet relationship insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createTriplets = [ \\\n",
    "    \"\"\"${0} isa graphTriplet has name \"{1}\";\n",
    "    (src-vertex-owned: ${2}, dst-vertex-owned: ${3}, edge-owned: ${4}, object-owner: ${0}) isa has-graphobjects;\n",
    "    \"\"\".format( \\\n",
    "            a['src_type']+\"_\"+a['relationship']+\"_\"+a['dst_type'], \\\n",
    "            a['src_type']+\" \"+a['relationship']+\" \"+a['dst_type'], \\\n",
    "            a['src_type'], \\\n",
    "            a['dst_type'], \\\n",
    "            a['relationship'] \\\n",
    "        ).replace(\"\\n\",\"\").replace(\"\\t\", ' ')\n",
    "    for a in tripleTypes \\\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$company_owned_by_company isa graphTriplet has name \"company owned_by company\";    (src-vertex-owned: $company, dst-vertex-owned: $company, edge-owned: $owned_by, object-owner: $company_owned_by_company) isa has-graphobjects;    '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createTriplets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$person_employed_by_company isa graphTriplet has name \"person employed_by company\";    (src-vertex-owned: $person, dst-vertex-owned: $company, edge-owned: $employed_by, object-owner: $person_employed_by_company) isa has-graphobjects;    '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createTriplets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Review of generated Graql statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verts = \" \".join(createVertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = \" \".join(createEdges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = \" \".join(createTriplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$person isa graphVertex has name \"person\"; $company isa graphVertex has name \"company\";\n",
      "$owned_by isa graphEdge has name \"owned_by\"; $employed_by isa graphEdge has name \"employed_by\"; $claims_dependent isa graphEdge has name \"claims_dependent\";\n",
      "$company_owned_by_company isa graphTriplet has name \"company owned_by company\";    (src-vertex-owned: $company, dst-vertex-owned: $company, edge-owned: $owned_by, object-owner: $company_owned_by_company) isa has-graphobjects;     $person_employed_by_company isa graphTriplet has name \"person employed_by company\";    (src-vertex-owned: $person, dst-vertex-owned: $company, edge-owned: $employed_by, object-owner: $person_employed_by_company) isa has-graphobjects;     $person_claims_dependent_person isa graphTriplet has name \"person claims_dependent person\";    (src-vertex-owned: $person, dst-vertex-owned: $person, edge-owned: $claims_dependent, object-owner: $person_claims_dependent_person) isa has-graphobjects;    \n"
     ]
    }
   ],
   "source": [
    "print(verts + '\\n' + edges + \"\\n\" + trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Company-[owned_by]->Person\n",
    "To see graph evolution in action, we will add a new triple using existing vertices and edges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "match\n",
    "    $person isa graphVertex has name \"person\"; $company isa graphVertex has name \"company\";\n",
    "    $owned_by isa graphEdge has name \"owned_by\";\n",
    "insert\n",
    "    $company_owned_by_person isa graphTriplet has name \"company owned_by person\";\n",
    "    (src-vertex-owned: $company, dst-vertex-owned: $person, edge-owned: $owned_by, object-owner: $company_owned_by_person) isa has-graphobjects;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
